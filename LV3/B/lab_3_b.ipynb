{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Laboratory Exercise: Poker Hand Classification from 5 Cards\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this laboratory exercise, you will build a **multi-class classification model** that predicts the **poker hand category** given **five playing cards**. Each data sample represents a complete 5-card poker hand, and the task is to correctly classify it into one of the standard poker hand types (e.g. *Nothing in hand*, *One pair*, *Straight*, *Flush*, *Royal flush*, etc.).\n",
    "\n",
    "You will implement a **full machine learning pipeline using PyTorch**, including data preprocessing, dataset definition, model building, training, evaluation, visualization, and final testing.\n",
    "\n",
    "This exercise focuses on **structured categorical data**, **multi-class classification**, and correct usage of **CrossEntropyLoss**.\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "- **Task:** Multiclass classification\n",
    "- **Target column:** `Hand`\n",
    "- **Goal:** Predict what will the winning hand be\n",
    "\n",
    "You will work with a provided dataset (`dataset.csv`) and implement a complete **machine learning training pipeline** using PyTorch.\n",
    "\n",
    "## Tasks Overview\n",
    "\n",
    "You are required to implement the following components:\n",
    "\n",
    "1. **Data Preparation**\n",
    "   - Load the `dataset.csv` file\n",
    "   - Separate features from the target column `Hand`\n",
    "   - Split the data into training, validation, and test sets\n",
    "   - Apply any required preprocessing\n",
    "\n",
    "2. **Dataset Class**\n",
    "   - Implement an `PockerDataset` class compatible with PyTorch’s `DataLoader`\n",
    "\n",
    "3. **Model Building**\n",
    "   - Implement a `build_model` function that returns a neural network for multiclass classification\n",
    "\n",
    "4. **Training and Evaluation**\n",
    "   - Implement:\n",
    "     - `train_one_epoch`\n",
    "     - `evaluate`\n",
    "     - `test`\n",
    "   - Train the model for a fixed number of epochs\n",
    "   - Track training loss, validation loss, and validation accuracy for each epoch\n",
    "\n",
    "5. **Visualization**\n",
    "   - Plot:\n",
    "     - Training loss vs. epochs\n",
    "     - Validation loss vs. epochs\n",
    "     - Validation accuracy vs. epochs\n",
    "\n",
    "6. **Testing and Reporting**\n",
    "   - Evaluate the final model on the test dataset\n",
    "   - Generate a **classification report** (precision, recall, F1-score)\n",
    "\n",
    "\n",
    "## Model Comparison Requirement\n",
    "\n",
    "You must design and train **two different model configurations**, for example:\n",
    "- Different network depths or widths\n",
    "- Different activation functions\n",
    "- Different regularization strategies (e.g. dropout)\n",
    "\n",
    "For **each model**, you must:\n",
    "- Train it for the same number of epochs\n",
    "- Plot training and validation metrics\n",
    "- Evaluate it on the test set"
   ],
   "id": "2a655134f9920125"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset Description\n",
    "\n",
    "You are given a CSV dataset with the following structure:\n",
    "\n",
    "```text\n",
    "Card 1,Card 2,Card 3,Card 4,Card 5,Hand\n",
    "Jack Spades,King Spades,10 Spades,Queen Spades,Ace Spades,Royal flush\n",
    "Queen Diamonds,Jack Diamonds,King Diamonds,10 Diamonds,Ace Diamonds,Royal flush\n",
    "2 Heart,4 Heart,5 Heart,3 Heart,6 Heart,Straight flush\n",
    "Ace Heart,Ace Spades,9 Diamonds,5 Heart,3 Spades,One pair\n",
    "```"
   ],
   "id": "7dc846eb9853272e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T12:40:05.283484Z",
     "start_time": "2025-12-15T12:40:05.268179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "2d78fb6449131e70",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T12:40:06.647362Z",
     "start_time": "2025-12-15T12:40:06.633650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "def prepare_data(df: pd.DataFrame) -> Tuple[\n",
    "    pd.DataFrame, pd.DataFrame, pd.DataFrame,\n",
    "    pd.DataFrame, pd.DataFrame, pd.DataFrame,\n",
    "    ColumnTransformer\n",
    "]:\n",
    "    \"\"\"\n",
    "    Prepare the poker hands dataset for training and evaluation.\n",
    "\n",
    "    The input DataFrame contains five cards describing a complete poker hand\n",
    "    and a categorical target column \"Hand\" indicating the poker hand type\n",
    "    (e.g. One pair, Straight, Flush, Royal flush).\n",
    "\n",
    "    Steps (you MUST follow these steps):\n",
    "    1. Split each card column (\"Card 1\" to \"Card 5\") into:\n",
    "       - Rank (2–10, Jack, Queen, King, Ace)\n",
    "       - Suit (Clubs, Diamonds, Hearts, Spades)\n",
    "    2. Encode card features:\n",
    "       - Ordinal-encode ranks using poker order\n",
    "       - Label encode suits\n",
    "    3. Separate features (X) and target (y), where the target column is \"Hand\".\n",
    "    4. Label-encode the target labels into integer class indices.\n",
    "    5. Use a ColumnTransformer to apply the encoders to all card features.\n",
    "    6. Fit the preprocessor.\n",
    "    7. Split the data in TWO stages (keep stratification):\n",
    "       - First split into train and test:\n",
    "            * test_size = 0.2\n",
    "            * random_state = 42\n",
    "            * stratify = y\n",
    "       - Then split the training part into train and validation:\n",
    "            * test_size = 0.2   (20% of the training set)\n",
    "            * random_state = 42\n",
    "            * stratify = y_train\n",
    "    8. Return:\n",
    "         X_train, X_val, X_test, y_train, y_val, y_test, preprocessor\n",
    "\n",
    "    Notes:\n",
    "    - The returned X arrays must be fully numeric.\n",
    "    - The returned y arrays must contain integer class labels.\n",
    "    - The data must be suitable for PyTorch multi-class classification.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    card_cols = [\"Card 1\", \"Card 2\", \"Card 3\", \"Card 4\", \"Card 5\"]\n",
    "    for c in card_cols:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {c}\")\n",
    "    if \"Hand\" not in df.columns:\n",
    "        raise ValueError('Missing required target column: \"Hand\"')\n",
    "\n",
    "    # --- 1) Split each card into Rank and Suit ---\n",
    "    # Supports both singular and plural suits (e.g., \"Heart\" or \"Hearts\")\n",
    "    suit_map = {\n",
    "        \"club\": \"Clubs\", \"clubs\": \"Clubs\",\n",
    "        \"diamond\": \"Diamonds\", \"diamonds\": \"Diamonds\",\n",
    "        \"heart\": \"Hearts\", \"hearts\": \"Hearts\",\n",
    "        \"spade\": \"Spades\", \"spades\": \"Spades\",\n",
    "    }\n",
    "\n",
    "    def parse_card(s: str) -> tuple[str, str]:\n",
    "        if pd.isna(s):\n",
    "            return (\"\", \"\")\n",
    "        parts = str(s).strip().split()\n",
    "        if len(parts) < 2:\n",
    "            raise ValueError(f\"Invalid card format: {s!r}\")\n",
    "        rank = \" \".join(parts[:-1]).strip()  # usually one token, but safer\n",
    "        suit_raw = parts[-1].strip().lower()\n",
    "        suit = suit_map.get(suit_raw, None)\n",
    "        if suit is None:\n",
    "            raise ValueError(f\"Unknown suit {parts[-1]!r} in card {s!r}\")\n",
    "        return (rank, suit)\n",
    "\n",
    "    rank_cols, suit_cols = [], []\n",
    "    for i, c in enumerate(card_cols, start=1):\n",
    "        rcol = f\"R{i}\"\n",
    "        scol = f\"S{i}\"\n",
    "        parsed = df[c].apply(parse_card)\n",
    "        df[rcol] = parsed.apply(lambda t: t[0])\n",
    "        df[scol] = parsed.apply(lambda t: t[1])\n",
    "        rank_cols.append(rcol)\n",
    "        suit_cols.append(scol)\n",
    "\n",
    "    # --- 3) Separate features/target ---\n",
    "    X = df[rank_cols + suit_cols]\n",
    "    y = df[\"Hand\"]\n",
    "\n",
    "    # --- 4) Label-encode target ---\n",
    "    y_encoder = LabelEncoder()\n",
    "    y_encoded = pd.Series(y_encoder.fit_transform(y), name=\"Hand\")\n",
    "\n",
    "    # --- 2 + 5) Encoders via ColumnTransformer ---\n",
    "    # Poker order for ranks:\n",
    "    rank_order = [[\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\",\n",
    "                   \"Jack\", \"Queen\", \"King\", \"Ace\"]]\n",
    "    suit_order = [[\"Clubs\", \"Diamonds\", \"Hearts\", \"Spades\"]]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"rank_ord\", OrdinalEncoder(categories=rank_order, handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "             rank_cols),\n",
    "            (\"suit_lbl\", OrdinalEncoder(categories=suit_order, handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "             suit_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "    # --- 6) Fit preprocessor ---\n",
    "    preprocessor.fit(X)\n",
    "\n",
    "    # Transform to numeric matrix; wrap as DataFrame for consistent return type\n",
    "    X_numeric = pd.DataFrame(preprocessor.transform(X), columns=rank_cols + suit_cols, index=X.index)\n",
    "\n",
    "    # --- 7) Two-stage split with stratification ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_numeric, y_encoded,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_train\n",
    "    )\n",
    "\n",
    "    # --- 8) Return ---\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, preprocessor"
   ],
   "id": "d50c90ff40e960eb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T12:40:22.116113Z",
     "start_time": "2025-12-15T12:40:22.077565Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv(\"dataset.csv\")",
   "id": "94d175da5ba6d4b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T12:40:25.452172Z",
     "start_time": "2025-12-15T12:40:25.442263Z"
    }
   },
   "cell_type": "code",
   "source": "df",
   "id": "879c55ad8550ca53",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               Card 1         Card 2         Card 3        Card 4  \\\n",
       "0         Jack Spades    King Spades      10 Spades  Queen Spades   \n",
       "1      Queen Diamonds  Jack Diamonds  King Diamonds   10 Diamonds   \n",
       "2            10 Clubs     Jack Clubs      Ace Clubs    King Clubs   \n",
       "3           Ace Clubs     King Clubs    Queen Clubs    Jack Clubs   \n",
       "4             2 Heart        4 Heart        5 Heart       3 Heart   \n",
       "...               ...            ...            ...           ...   \n",
       "25004      9 Diamonds       6 Spades     Jack Clubs   Queen Clubs   \n",
       "25005       Ace Clubs       10 Clubs  King Diamonds    4 Diamonds   \n",
       "25006      Ace Spades      10 Spades        4 Clubs     Ace Clubs   \n",
       "25007    Queen Spades        3 Clubs       10 Heart   Queen Heart   \n",
       "25008         7 Heart  Jack Diamonds     3 Diamonds       8 Clubs   \n",
       "\n",
       "             Card 5             Hand  \n",
       "0        Ace Spades      Royal flush  \n",
       "1      Ace Diamonds      Royal flush  \n",
       "2       Queen Clubs      Royal flush  \n",
       "3          10 Clubs      Royal flush  \n",
       "4           6 Heart   Straight flush  \n",
       "...             ...              ...  \n",
       "25004      4 Spades  Nothing in hand  \n",
       "25005      10 Heart         One pair  \n",
       "25006    King Clubs         One pair  \n",
       "25007       9 Clubs         One pair  \n",
       "25008    7 Diamonds         One pair  \n",
       "\n",
       "[25009 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Card 1</th>\n",
       "      <th>Card 2</th>\n",
       "      <th>Card 3</th>\n",
       "      <th>Card 4</th>\n",
       "      <th>Card 5</th>\n",
       "      <th>Hand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jack Spades</td>\n",
       "      <td>King Spades</td>\n",
       "      <td>10 Spades</td>\n",
       "      <td>Queen Spades</td>\n",
       "      <td>Ace Spades</td>\n",
       "      <td>Royal flush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Queen Diamonds</td>\n",
       "      <td>Jack Diamonds</td>\n",
       "      <td>King Diamonds</td>\n",
       "      <td>10 Diamonds</td>\n",
       "      <td>Ace Diamonds</td>\n",
       "      <td>Royal flush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 Clubs</td>\n",
       "      <td>Jack Clubs</td>\n",
       "      <td>Ace Clubs</td>\n",
       "      <td>King Clubs</td>\n",
       "      <td>Queen Clubs</td>\n",
       "      <td>Royal flush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ace Clubs</td>\n",
       "      <td>King Clubs</td>\n",
       "      <td>Queen Clubs</td>\n",
       "      <td>Jack Clubs</td>\n",
       "      <td>10 Clubs</td>\n",
       "      <td>Royal flush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 Heart</td>\n",
       "      <td>4 Heart</td>\n",
       "      <td>5 Heart</td>\n",
       "      <td>3 Heart</td>\n",
       "      <td>6 Heart</td>\n",
       "      <td>Straight flush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>9 Diamonds</td>\n",
       "      <td>6 Spades</td>\n",
       "      <td>Jack Clubs</td>\n",
       "      <td>Queen Clubs</td>\n",
       "      <td>4 Spades</td>\n",
       "      <td>Nothing in hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25005</th>\n",
       "      <td>Ace Clubs</td>\n",
       "      <td>10 Clubs</td>\n",
       "      <td>King Diamonds</td>\n",
       "      <td>4 Diamonds</td>\n",
       "      <td>10 Heart</td>\n",
       "      <td>One pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25006</th>\n",
       "      <td>Ace Spades</td>\n",
       "      <td>10 Spades</td>\n",
       "      <td>4 Clubs</td>\n",
       "      <td>Ace Clubs</td>\n",
       "      <td>King Clubs</td>\n",
       "      <td>One pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25007</th>\n",
       "      <td>Queen Spades</td>\n",
       "      <td>3 Clubs</td>\n",
       "      <td>10 Heart</td>\n",
       "      <td>Queen Heart</td>\n",
       "      <td>9 Clubs</td>\n",
       "      <td>One pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25008</th>\n",
       "      <td>7 Heart</td>\n",
       "      <td>Jack Diamonds</td>\n",
       "      <td>3 Diamonds</td>\n",
       "      <td>8 Clubs</td>\n",
       "      <td>7 Diamonds</td>\n",
       "      <td>One pair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25009 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T12:40:43.986105Z",
     "start_time": "2025-12-15T12:40:43.427789Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, X_val, X_test, y_train, y_val, y_test, preprocessor = prepare_data(df)",
   "id": "8d951ee441060a04",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch: if categories is an array, it has to be of shape (n_features,).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m X_train, X_val, X_test, y_train, y_val, y_test, preprocessor = \u001B[43mprepare_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 116\u001B[39m, in \u001B[36mprepare_data\u001B[39m\u001B[34m(df)\u001B[39m\n\u001B[32m    104\u001B[39m preprocessor = ColumnTransformer(\n\u001B[32m    105\u001B[39m     transformers=[\n\u001B[32m    106\u001B[39m         (\u001B[33m\"\u001B[39m\u001B[33mrank_ord\u001B[39m\u001B[33m\"\u001B[39m, OrdinalEncoder(categories=rank_order, handle_unknown=\u001B[33m\"\u001B[39m\u001B[33muse_encoded_value\u001B[39m\u001B[33m\"\u001B[39m, unknown_value=-\u001B[32m1\u001B[39m),\n\u001B[32m   (...)\u001B[39m\u001B[32m    112\u001B[39m     verbose_feature_names_out=\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    113\u001B[39m )\n\u001B[32m    115\u001B[39m \u001B[38;5;66;03m# --- 6) Fit preprocessor ---\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m \u001B[43mpreprocessor\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[38;5;66;03m# Transform to numeric matrix; wrap as DataFrame for consistent return type\u001B[39;00m\n\u001B[32m    119\u001B[39m X_numeric = pd.DataFrame(preprocessor.transform(X), columns=rank_cols + suit_cols, index=X.index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:934\u001B[39m, in \u001B[36mColumnTransformer.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m    931\u001B[39m _raise_for_params(params, \u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mfit\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    932\u001B[39m \u001B[38;5;66;03m# we use fit_transform to make sure to set sparse_output_ (for which we\u001B[39;00m\n\u001B[32m    933\u001B[39m \u001B[38;5;66;03m# need the transformed data) to have consistent output type in predict\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m934\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m=\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    935\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001B[39m, in \u001B[36m_wrap_method_output.<locals>.wrapped\u001B[39m\u001B[34m(self, X, *args, **kwargs)\u001B[39m\n\u001B[32m    314\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m     data_to_wrap = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    317\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    318\u001B[39m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[32m    319\u001B[39m         return_tuple = (\n\u001B[32m    320\u001B[39m             _wrap_data_with_container(method, data_to_wrap[\u001B[32m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[32m    321\u001B[39m             *data_to_wrap[\u001B[32m1\u001B[39m:],\n\u001B[32m    322\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/sklearn/base.py:1365\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1358\u001B[39m     estimator._validate_params()\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1361\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1362\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1363\u001B[39m     )\n\u001B[32m   1364\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:996\u001B[39m, in \u001B[36mColumnTransformer.fit_transform\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m    993\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    994\u001B[39m     routed_params = \u001B[38;5;28mself\u001B[39m._get_empty_routing()\n\u001B[32m--> \u001B[39m\u001B[32m996\u001B[39m result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_func_on_transformers\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    997\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    998\u001B[39m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    999\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_fit_transform_one\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1000\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcolumn_as_labels\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1001\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1002\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1004\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m result:\n\u001B[32m   1005\u001B[39m     \u001B[38;5;28mself\u001B[39m._update_fitted_transformers([])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:897\u001B[39m, in \u001B[36mColumnTransformer._call_func_on_transformers\u001B[39m\u001B[34m(self, X, y, func, column_as_labels, routed_params)\u001B[39m\n\u001B[32m    885\u001B[39m             extra_args = {}\n\u001B[32m    886\u001B[39m         jobs.append(\n\u001B[32m    887\u001B[39m             delayed(func)(\n\u001B[32m    888\u001B[39m                 transformer=clone(trans) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fitted \u001B[38;5;28;01melse\u001B[39;00m trans,\n\u001B[32m   (...)\u001B[39m\u001B[32m    894\u001B[39m             )\n\u001B[32m    895\u001B[39m         )\n\u001B[32m--> \u001B[39m\u001B[32m897\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    899\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    900\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mExpected 2D array, got 1D array instead\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/sklearn/utils/parallel.py:82\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     73\u001B[39m warning_filters = warnings.filters\n\u001B[32m     74\u001B[39m iterable_with_config_and_warning_filters = (\n\u001B[32m     75\u001B[39m     (\n\u001B[32m     76\u001B[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001B[32m   (...)\u001B[39m\u001B[32m     80\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     81\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config_and_warning_filters\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/joblib/parallel.py:1986\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   1984\u001B[39m     output = \u001B[38;5;28mself\u001B[39m._get_sequential_output(iterable)\n\u001B[32m   1985\u001B[39m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m1986\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1988\u001B[39m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[32m   1989\u001B[39m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[32m   1990\u001B[39m \u001B[38;5;66;03m# reused, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[32m   1991\u001B[39m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[32m   1992\u001B[39m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[32m   1993\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._lock:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/joblib/parallel.py:1914\u001B[39m, in \u001B[36mParallel._get_sequential_output\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   1912\u001B[39m \u001B[38;5;28mself\u001B[39m.n_dispatched_batches += \u001B[32m1\u001B[39m\n\u001B[32m   1913\u001B[39m \u001B[38;5;28mself\u001B[39m.n_dispatched_tasks += \u001B[32m1\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1914\u001B[39m res = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1915\u001B[39m \u001B[38;5;28mself\u001B[39m.n_completed_tasks += \u001B[32m1\u001B[39m\n\u001B[32m   1916\u001B[39m \u001B[38;5;28mself\u001B[39m.print_progress()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/sklearn/utils/parallel.py:147\u001B[39m, in \u001B[36m_FuncWrapper.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(**config), warnings.catch_warnings():\n\u001B[32m    146\u001B[39m     warnings.filters = warning_filters\n\u001B[32m--> \u001B[39m\u001B[32m147\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/sklearn/pipeline.py:1540\u001B[39m, in \u001B[36m_fit_transform_one\u001B[39m\u001B[34m(transformer, X, y, weight, message_clsname, message, params)\u001B[39m\n\u001B[32m   1538\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(message_clsname, message):\n\u001B[32m   1539\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(transformer, \u001B[33m\"\u001B[39m\u001B[33mfit_transform\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1540\u001B[39m         res = \u001B[43mtransformer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfit_transform\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1541\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1542\u001B[39m         res = transformer.fit(X, y, **params.get(\u001B[33m\"\u001B[39m\u001B[33mfit\u001B[39m\u001B[33m\"\u001B[39m, {})).transform(\n\u001B[32m   1543\u001B[39m             X, **params.get(\u001B[33m\"\u001B[39m\u001B[33mtransform\u001B[39m\u001B[33m\"\u001B[39m, {})\n\u001B[32m   1544\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001B[39m, in \u001B[36m_wrap_method_output.<locals>.wrapped\u001B[39m\u001B[34m(self, X, *args, **kwargs)\u001B[39m\n\u001B[32m    314\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m     data_to_wrap = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    317\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    318\u001B[39m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[32m    319\u001B[39m         return_tuple = (\n\u001B[32m    320\u001B[39m             _wrap_data_with_container(method, data_to_wrap[\u001B[32m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[32m    321\u001B[39m             *data_to_wrap[\u001B[32m1\u001B[39m:],\n\u001B[32m    322\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/sklearn/base.py:894\u001B[39m, in \u001B[36mTransformerMixin.fit_transform\u001B[39m\u001B[34m(self, X, y, **fit_params)\u001B[39m\n\u001B[32m    879\u001B[39m         warnings.warn(\n\u001B[32m    880\u001B[39m             (\n\u001B[32m    881\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mThis object (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) has a `transform`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    889\u001B[39m             \u001B[38;5;167;01mUserWarning\u001B[39;00m,\n\u001B[32m    890\u001B[39m         )\n\u001B[32m    892\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    893\u001B[39m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m894\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m.transform(X)\n\u001B[32m    895\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    896\u001B[39m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[32m    897\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/sklearn/base.py:1365\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1358\u001B[39m     estimator._validate_params()\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1361\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1362\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1363\u001B[39m     )\n\u001B[32m   1364\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:1515\u001B[39m, in \u001B[36mOrdinalEncoder.fit\u001B[39m\u001B[34m(self, X, y)\u001B[39m\n\u001B[32m   1508\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[32m   1509\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33munknown_value should only be set when \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1510\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mhandle_unknown is \u001B[39m\u001B[33m'\u001B[39m\u001B[33muse_encoded_value\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1511\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mgot \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.unknown_value\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1512\u001B[39m     )\n\u001B[32m   1514\u001B[39m \u001B[38;5;66;03m# `_fit` will only raise an error when `self.handle_unknown=\"error\"`\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1515\u001B[39m fit_results = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1516\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1517\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhandle_unknown\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhandle_unknown\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1518\u001B[39m \u001B[43m    \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mallow-nan\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1519\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_and_ignore_missing_for_infrequent\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1520\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1521\u001B[39m \u001B[38;5;28mself\u001B[39m._missing_indices = fit_results[\u001B[33m\"\u001B[39m\u001B[33mmissing_indices\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1523\u001B[39m cardinalities = [\u001B[38;5;28mlen\u001B[39m(categories) \u001B[38;5;28;01mfor\u001B[39;00m categories \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.categories_]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/data-science/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:90\u001B[39m, in \u001B[36m_BaseEncoder._fit\u001B[39m\u001B[34m(self, X, handle_unknown, ensure_all_finite, return_counts, return_and_ignore_missing_for_infrequent)\u001B[39m\n\u001B[32m     88\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.categories != \u001B[33m\"\u001B[39m\u001B[33mauto\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     89\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m.categories) != n_features:\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m     91\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mShape mismatch: if categories is an array,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     92\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m it has to be of shape (n_features,).\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     93\u001B[39m         )\n\u001B[32m     95\u001B[39m \u001B[38;5;28mself\u001B[39m.categories_ = []\n\u001B[32m     96\u001B[39m category_counts = []\n",
      "\u001B[31mValueError\u001B[39m: Shape mismatch: if categories is an array, it has to be of shape (n_features,)."
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T09:43:29.151742Z",
     "start_time": "2025-12-12T09:43:29.148206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PockerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for poker hand classification.\n",
    "\n",
    "    Each sample consists of:\n",
    "    - a numeric feature vector representing five playing cards\n",
    "    - a multiclass label indicating the poker hand category\n",
    "\n",
    "    Requirements:\n",
    "    - __init__(self, X, y):\n",
    "        * X: numpy array of numeric features\n",
    "        * y: array-like of integer class labels\n",
    "        * Store:\n",
    "            - X as a float32 tensor\n",
    "            - y as a long tensor (class indices)\n",
    "    - __len__(self):\n",
    "        * Return the number of samples\n",
    "    - __getitem__(self, idx):\n",
    "        * Return (X[idx], y[idx])\n",
    "    \"\"\"\n",
    "    pass"
   ],
   "id": "33ad6ce494e276ff",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T09:43:29.421324Z",
     "start_time": "2025-12-12T09:43:29.417695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model: nn.Module,\n",
    "                    train_loader: DataLoader,\n",
    "                    criterion,\n",
    "                    optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Train the model for ONE epoch on the training dataset.\n",
    "\n",
    "    This is a multi-class classification task for poker hand prediction.\n",
    "\n",
    "    Requirements:\n",
    "    - Set the model to training mode using model.train()\n",
    "    - Iterate over batches from train_loader\n",
    "    - For each batch:\n",
    "        * Compute model outputs (logits)\n",
    "        * Compute the loss using CrossEntropyLoss\n",
    "        * Zero the gradients\n",
    "        * Perform backpropagation\n",
    "        * Update model parameters using the optimizer\n",
    "    - Accumulate the training loss over all batches\n",
    "    - Return the average training loss as a float\n",
    "      (total loss divided by the number of batches)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ],
   "id": "3016b59eba365eee",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T09:43:29.647899Z",
     "start_time": "2025-12-12T09:43:29.644227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model: nn.Module,\n",
    "             val_loader: DataLoader,\n",
    "             criterion: nn.Module) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation dataset.\n",
    "\n",
    "    This is a multi-class poker hand classification task.\n",
    "\n",
    "    Requirements:\n",
    "    - Set the model to evaluation mode using model.eval()\n",
    "    - Disable gradient computation using torch.no_grad()\n",
    "    - Iterate over batches from val_loader\n",
    "    - For each batch:\n",
    "        * Compute model outputs (logits)\n",
    "        * Compute and accumulate validation loss\n",
    "        * Convert logits to predicted class labels using argmax\n",
    "        * Collect predicted labels and true labels\n",
    "    - Compute validation accuracy over the entire validation set\n",
    "    - Return:\n",
    "        - validation accuracy (float)\n",
    "        - validation loss (float)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ],
   "id": "31262b08d1b91b19",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T09:43:29.881949Z",
     "start_time": "2025-12-12T09:43:29.878565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(model: nn.Module,\n",
    "         test_loader: DataLoader) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on the test dataset.\n",
    "\n",
    "    This function performs inference for multi-class poker hand classification.\n",
    "\n",
    "    Requirements:\n",
    "    - Set the model to evaluation mode using model.eval()\n",
    "    - Disable gradient computation using torch.no_grad()\n",
    "    - Iterate over batches from test_loader\n",
    "    - For each batch:\n",
    "        * Compute model outputs (logits)\n",
    "        * Convert logits to predicted class labels using argmax\n",
    "        * Collect all predicted labels and true labels\n",
    "    - Return:\n",
    "        - Tensor of true labels (shape: N,)\n",
    "        - Tensor of predicted labels (shape: N,)\n",
    "\n",
    "    These outputs will be used to compute a classification report.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ],
   "id": "a2b82069d52bb35a",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T09:43:30.089907Z",
     "start_time": "2025-12-12T09:43:30.087441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_model_1(input_dim: int) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Build and return a PyTorch neural network for poker hand classification.\n",
    "\n",
    "    Requirements:\n",
    "    - Use nn.Sequential to define the model\n",
    "    - The model must accept input vectors of size input_dim\n",
    "    - The final layer must output logits for all poker hand classes\n",
    "    - Do NOT apply Softmax in the final layer\n",
    "\n",
    "    Note:\n",
    "    - Use CrossEntropyLoss during training\n",
    "    - You may include additional hidden layers, activations, or regularization\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ],
   "id": "7c371af5bc7097ec",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T09:43:30.306472Z",
     "start_time": "2025-12-12T09:43:30.303344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_model_2(input_dim: int) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Build and return a second PyTorch neural network for poker hand classification.\n",
    "\n",
    "    This model should differ from build_model_1 (e.g. depth, width, dropout).\n",
    "\n",
    "    Requirements:\n",
    "    - Use nn.Sequential to define the model\n",
    "    - The model must accept input vectors of size input_dim\n",
    "    - The final layer must output logits for all poker hand classes\n",
    "    - Do NOT apply Softmax in the final layer\n",
    "\n",
    "    Note:\n",
    "    - Use CrossEntropyLoss during training\n",
    "    - This model will be compared against build_model_1\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ],
   "id": "bea098a0a0accc59",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build the models",
   "id": "9c72fd94cdd4ba30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T09:43:30.725278Z",
     "start_time": "2025-12-12T09:43:30.723555Z"
    }
   },
   "cell_type": "code",
   "source": "# Call the build functions",
   "id": "9145d1336a4f4844",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train model 1",
   "id": "cda9a7bcc172faff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T09:43:31.158851Z",
     "start_time": "2025-12-12T09:43:31.156257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 0\n",
    "train_losses_1 = []\n",
    "val_losses_1 = []\n",
    "val_accuracies_1 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Call all required functions and store the computed metrics\n",
    "    # (training loss, validation loss, and validation accuracy).\n",
    "\n",
    "    train_loss =0\n",
    "    val_acc = 0\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | Train loss: {train_loss:.4f} | Val acc: {val_acc:.4f}\")"
   ],
   "id": "6dafc2594240ea8b",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train model 2",
   "id": "ccbdbe9b9307d8fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T09:43:31.567497Z",
     "start_time": "2025-12-12T09:43:31.564502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 0\n",
    "train_losses_2 = []\n",
    "val_losses_2 = []\n",
    "val_accuracies_2 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Call all required functions and store the computed metrics\n",
    "    # (training loss, validation loss, and validation accuracy).\n",
    "\n",
    "    train_loss =0\n",
    "    val_acc = 0\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | Train loss: {train_loss:.4f} | Val acc: {val_acc:.4f}\")"
   ],
   "id": "a372f48c2092c613",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualize",
   "id": "f9eac2e9cd3cf5ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T09:43:32.041493Z",
     "start_time": "2025-12-12T09:43:32.038142Z"
    }
   },
   "cell_type": "code",
   "source": "# Visualize training and validation loss on the same plot, and visualize the validation accuracy across epochs.",
   "id": "c559c663e368e087",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluate",
   "id": "3e5216ba01d97102"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T09:43:32.482083Z",
     "start_time": "2025-12-12T09:43:32.480319Z"
    }
   },
   "cell_type": "code",
   "source": "# Evaluate on the test dataset",
   "id": "c5b0a8ab4e73f4de",
   "outputs": [],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
